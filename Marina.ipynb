{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda (GPU support) is not available :(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np              # NumPy, for working with arrays/tensors \n",
    "import time                     # For measuring time\n",
    "import random                   # Python's random library\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# PyTorch libraries:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print(\"Cuda (GPU support) is available and enabled!\")\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  print(\"Cuda (GPU support) is not available :(\")\n",
    "  device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_FILEPATH = \"./SMAP MSL/labeled_anomalies.csv\"\n",
    "TRAINSET_FILEPATH = \"./SMAP MSL/data/data/train\"\n",
    "TESTSET_FILEPATH = \"./SMAP MSL/data/data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82 entries, 0 to 81\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   chan_id            82 non-null     object\n",
      " 1   spacecraft         82 non-null     object\n",
      " 2   anomaly_sequences  82 non-null     object\n",
      " 3   class              82 non-null     object\n",
      " 4   num_values         82 non-null     int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 3.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chan_id</th>\n",
       "      <th>spacecraft</th>\n",
       "      <th>anomaly_sequences</th>\n",
       "      <th>class</th>\n",
       "      <th>num_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P-1</td>\n",
       "      <td>SMAP</td>\n",
       "      <td>[[2149, 2349], [4536, 4844], [3539, 3779]]</td>\n",
       "      <td>[contextual, contextual, contextual]</td>\n",
       "      <td>8505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S-1</td>\n",
       "      <td>SMAP</td>\n",
       "      <td>[[5300, 5747]]</td>\n",
       "      <td>[point]</td>\n",
       "      <td>7331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E-1</td>\n",
       "      <td>SMAP</td>\n",
       "      <td>[[5000, 5030], [5610, 6086]]</td>\n",
       "      <td>[contextual, contextual]</td>\n",
       "      <td>8516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E-2</td>\n",
       "      <td>SMAP</td>\n",
       "      <td>[[5598, 6995]]</td>\n",
       "      <td>[point]</td>\n",
       "      <td>8532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E-3</td>\n",
       "      <td>SMAP</td>\n",
       "      <td>[[5094, 8306]]</td>\n",
       "      <td>[point]</td>\n",
       "      <td>8307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chan_id spacecraft                           anomaly_sequences   \n",
       "0     P-1       SMAP  [[2149, 2349], [4536, 4844], [3539, 3779]]  \\\n",
       "1     S-1       SMAP                              [[5300, 5747]]   \n",
       "2     E-1       SMAP                [[5000, 5030], [5610, 6086]]   \n",
       "3     E-2       SMAP                              [[5598, 6995]]   \n",
       "4     E-3       SMAP                              [[5094, 8306]]   \n",
       "\n",
       "                                  class  num_values  \n",
       "0  [contextual, contextual, contextual]        8505  \n",
       "1                               [point]        7331  \n",
       "2              [contextual, contextual]        8516  \n",
       "3                               [point]        8532  \n",
       "4                               [point]        8307  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(LABELS_FILEPATH)\n",
    "labels.info()\n",
    "labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading \n",
    "# T-10 data is not in Labels but in Train Dataset. Therefore, T-10 will not be used for now.\n",
    "import os\n",
    "train_datas = {}\n",
    "test_datas = {}\n",
    "arranged_train_datas = {}\n",
    "arranged_test_datas = {}\n",
    "\n",
    "desiredSC      = 'SMAP'\n",
    "# Iterate directory\n",
    "for path in os.listdir(TRAINSET_FILEPATH):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(TRAINSET_FILEPATH, path)):\n",
    "        if path != 'T-10.npy':\n",
    "            SC_labels     = labels[labels['chan_id'] == path.replace('.npy','')]\n",
    "            SC            = SC_labels['spacecraft']\n",
    "            # print(\"SC_labels\",SC_labels)\n",
    "            # print(\"SC\",SC)\n",
    "            # print((SC.iloc[0]),path.replace('.npy',''))\n",
    "            SC_name       = SC.iloc[0]\n",
    "            if SC_name == desiredSC:\n",
    "                train_data = np.load(os.path.join(TRAINSET_FILEPATH, path))\n",
    "                train_datas[path] = train_data\n",
    "\n",
    "for path in os.listdir(TESTSET_FILEPATH):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(TESTSET_FILEPATH, path)):\n",
    "        if path != 'T-10.npy':\n",
    "            SC_labels     = labels[labels['chan_id'] == path.replace('.npy','')]\n",
    "            SC            = SC_labels['spacecraft']\n",
    "            SC_name       = SC.iloc[0]\n",
    "            if SC_name == desiredSC:       \n",
    "                test_data = np.load(os.path.join(TESTSET_FILEPATH, path))\n",
    "                test_datas[path] = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Writing Functions\n",
    "\n",
    "def windowed_Set(original_data, window_size, shifting,horizon):\n",
    "    time_sequence_length, n_dimensions  = original_data.shape\n",
    "    T = int((time_sequence_length - window_size-horizon) / shifting + 1)\n",
    "    \n",
    "    windowedArray = np.empty((n_dimensions, T, window_size))\n",
    "    futuredArray  = np.empty((n_dimensions, T, horizon))\n",
    "\n",
    "    for d in range(n_dimensions):\n",
    "        for i in range(T):\n",
    "            start_index = i * shifting\n",
    "            window       = original_data[start_index : start_index + window_size,d]\n",
    "            horizon_data = original_data[start_index + window_size : start_index + window_size + horizon,d]\n",
    "            windowedArray[d, i, :] = window\n",
    "            futuredArray[d, i, :]  = horizon_data\n",
    "    return windowedArray, futuredArray\n",
    "\n",
    "def normalize_data(data_, desired_mean, desired_std):\n",
    "    \"\"\"This code turns the input into have the desired mean and variance values. When standard deviation is zero, \n",
    "       since division will be infinity, it will bypass.\n",
    "    \"\"\"\n",
    "    # Calculate the mean and variance of the original data\n",
    "    data = data_.copy()\n",
    "    original_mean = np.mean(data)\n",
    "    original_std = np.std(data)\n",
    "     \n",
    "    # Subtract the mean from each data point\n",
    "    data -= original_mean\n",
    "    # Divide each data point by the square root of the variance\n",
    "    if original_std != 0:\n",
    "        data /= original_std\n",
    "        # Multiply each data point by the desired standard deviation\n",
    "        data *= desired_std\n",
    "    # Add the desired mean to each data point\n",
    "    data += desired_mean\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total set number is: 54 \n",
      "1 Set name: T-3.npy \n",
      "Set shape: (2876, 25) [t,D] \n",
      "Windowed array shape: (25, 556, 100) [D,i,w] \n",
      "Futured array shape: (25, 556, 1) [D,i,zeta] \n",
      "One sample mean: 1.3242382001648598e-16 \n",
      "One sample dimension variance: 1.0\n"
     ]
    }
   ],
   "source": [
    "## Normalization Parameters\n",
    "INPUT_LENGTH = 100\n",
    "wL = INPUT_LENGTH     # window length\n",
    "zeta = 1             # horizon length (future length to predict)\n",
    "shift = 5\n",
    "\n",
    "#4.1 data normalization \n",
    "#1 - training normalization \n",
    "train_means = {}\n",
    "train_variances = {}\n",
    "windowedTrain = {}      # Dividing the data with windows\n",
    "futuredTrain = {}       # Creating arrays to be futured\n",
    "\n",
    "for train_data in train_datas:\n",
    "    #print(train_datas[train_data].shape)\n",
    "    D                                       = train_datas[train_data].shape[1]\n",
    "    iN                                      = train_datas[train_data].shape[0]\n",
    "\n",
    "    for D_ in range(D):\n",
    "        train_datas[train_data][:,D_]       = normalize_data(train_datas[train_data][:,D_],0,1)\n",
    "    #train_datas[train_data] = train_datas[train_data] * 2 - 1  # variance 1 olacak teyit et\n",
    "    \n",
    "    windowedTrain[train_data], futuredTrain[train_data] = windowed_Set(train_datas[train_data],wL,shift,zeta)\n",
    "\n",
    "print(\"Total set number is:\",len(windowedTrain),\n",
    "      \"\\n1 Set name:\",train_data,\n",
    "      \"\\nSet shape:\",train_datas[train_data].shape,\"[t,D]\",\n",
    "      \"\\nWindowed array shape:\",windowedTrain[train_data].shape,\"[D,i,w]\",\n",
    "      \"\\nFutured array shape:\",futuredTrain[train_data].shape,\"[D,i,zeta]\",\n",
    "      \"\\nOne sample mean:\", np.mean(train_datas[train_data]),\n",
    "      \"\\nOne sample dimension variance:\", np.std(train_datas[train_data][:,0]))\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining intermediate length and alpha before designing the modules\n",
    "INTERMEDIATE_LENGTH         = 24\n",
    "ALPHA                       = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window length: 100 \n",
      "Shifting: 5 \n",
      "Horizon (zeta): 1 \n",
      "Total set number is: 54 \n",
      "1 Set name: T-3.npy \n",
      "Set shape: (8579, 25) [t,D] \n",
      "Windowed array shape: (25, 1696, 100) [D,i,w] \n",
      "Futured array shape: (25, 1696, 1) [D,i,zeta] \n",
      "Test means array shape: (25, 1696) [D,i] \n",
      "Test variance array shape: (25, 1696) [D,i]\n"
     ]
    }
   ],
   "source": [
    "windowedTest    = {}\n",
    "futuredTest     = {}\n",
    "test_means      = {}\n",
    "test_variances  = {}\n",
    "test_std        = {}\n",
    "\n",
    "for test_data in test_datas:\n",
    "    windowedTest[test_data], futuredTest[test_data] = windowed_Set(test_datas[test_data],wL,shift,zeta)\n",
    "\n",
    "for test_data in test_datas:\n",
    "    #print(test_datas[test_data].shape)\n",
    "    D                               = windowedTest[test_data].shape[0]\n",
    "    iN                              = windowedTest[test_data].shape[1]\n",
    "\n",
    "    test_means[test_data]           = np.zeros((D,iN))\n",
    "    test_variances[test_data]       = np.zeros((D,iN))\n",
    "    test_std[test_data]             = np.zeros((D,iN))\n",
    "    test_means[test_data][:,0]      = 0\n",
    "    test_std[test_data][:,0]        = 0\n",
    "    test_variances[test_data][:,0]  = 1 \n",
    "\n",
    "    for iN_ in range(1,iN): \n",
    "        E                                = np.mean(windowedTest[test_data][:,iN_,:],axis = 1)\n",
    "        E2                               = np.mean(windowedTest[test_data][:,iN_,:]**2,axis = 1)    \n",
    "        test_means[test_data][:,iN_]     = (1-ALPHA)*test_means[test_data][:,iN_-1] + ALPHA*E \n",
    "        test_variances[test_data][:,iN_] = (1-ALPHA)*test_variances[test_data][:,iN_-1]+ALPHA*(E2-E**2)\n",
    "        test_std[test_data][:,iN_]       = np.sqrt(test_variances[test_data][:,iN_-1])\n",
    "        for D_ in range(D):\n",
    "            dynamicMean                       = test_means[test_data][D_,iN_-1]\n",
    "            dynamicVar                        = test_variances[test_data][D_,iN_-1]\n",
    "            dynamicStd                        = test_std[test_data][D_,iN_-1]\n",
    "            \n",
    "            windowedTest[test_data][D_,iN_,:] = normalize_data(windowedTest[test_data][D_,iN_,:],dynamicMean,dynamicStd)\n",
    "\n",
    "print(\"Window length:\", wL,\n",
    "      \"\\nShifting:\",shift,\n",
    "      \"\\nHorizon (zeta):\",zeta,\n",
    "      \"\\nTotal set number is:\",len(windowedTest),\n",
    "      \"\\n1 Set name:\",test_data,\n",
    "      \"\\nSet shape:\",test_datas[test_data].shape,\"[t,D]\",\n",
    "      \"\\nWindowed array shape:\",windowedTest[test_data].shape,\"[D,i,w]\",\n",
    "      \"\\nFutured array shape:\",futuredTest[test_data].shape,\"[D,i,zeta]\",\n",
    "      \"\\nTest means array shape:\",test_means[test_data].shape,\"[D,i]\",\n",
    "      \"\\nTest variance array shape:\",test_variances[test_data].shape,\"[D,i]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['A-1.npy', 'A-2.npy', 'A-3.npy', 'A-4.npy', 'A-5.npy', 'A-6.npy', 'A-7.npy', 'A-8.npy', 'A-9.npy', 'B-1.npy', 'D-1.npy', 'D-11.npy', 'D-12.npy', 'D-13.npy', 'D-2.npy', 'D-3.npy', 'D-4.npy', 'D-5.npy', 'D-6.npy', 'D-7.npy', 'D-8.npy', 'D-9.npy', 'E-1.npy', 'E-10.npy', 'E-11.npy', 'E-12.npy', 'E-13.npy', 'E-2.npy', 'E-3.npy', 'E-4.npy', 'E-5.npy', 'E-6.npy', 'E-7.npy', 'E-8.npy', 'E-9.npy', 'F-1.npy', 'F-2.npy', 'F-3.npy', 'G-1.npy', 'G-2.npy', 'G-3.npy', 'G-4.npy', 'G-6.npy', 'G-7.npy', 'P-1.npy', 'P-2.npy', 'P-3.npy', 'P-4.npy', 'P-7.npy', 'R-1.npy', 'S-1.npy', 'T-1.npy', 'T-2.npy', 'T-3.npy'])\n",
      "\n",
      " dict_keys(['A-1.npy', 'A-2.npy', 'A-3.npy', 'A-4.npy', 'A-5.npy', 'A-6.npy', 'A-7.npy', 'A-8.npy', 'A-9.npy', 'B-1.npy', 'D-1.npy', 'D-11.npy', 'D-12.npy', 'D-13.npy', 'D-2.npy', 'D-3.npy', 'D-4.npy', 'D-5.npy', 'D-6.npy', 'D-7.npy', 'D-8.npy', 'D-9.npy', 'E-1.npy', 'E-10.npy', 'E-11.npy', 'E-12.npy', 'E-13.npy', 'E-2.npy', 'E-3.npy', 'E-4.npy', 'E-5.npy', 'E-6.npy', 'E-7.npy', 'E-8.npy', 'E-9.npy', 'F-1.npy', 'F-2.npy', 'F-3.npy', 'G-1.npy', 'G-2.npy', 'G-3.npy', 'G-4.npy', 'G-6.npy', 'G-7.npy', 'P-1.npy', 'P-2.npy', 'P-3.npy', 'P-4.npy', 'P-7.npy', 'R-1.npy', 'S-1.npy', 'T-1.npy', 'T-2.npy', 'T-3.npy'])\n",
      "\n",
      " ['P-1' 'S-1' 'E-1' 'E-2' 'E-3' 'E-4' 'E-5' 'E-6' 'E-7' 'E-8' 'E-9' 'E-10'\n",
      " 'E-11' 'E-12' 'E-13' 'A-1' 'D-1' 'P-2' 'P-3' 'D-2' 'D-3' 'D-4' 'A-2'\n",
      " 'A-3' 'A-4' 'G-1' 'G-2' 'D-5' 'D-6' 'D-7' 'F-1' 'P-4' 'G-3' 'T-1' 'T-2'\n",
      " 'D-8' 'D-9' 'F-2' 'G-4' 'T-3' 'D-11' 'D-12' 'B-1' 'G-6' 'G-7' 'P-7' 'R-1'\n",
      " 'A-5' 'A-6' 'A-7' 'D-13' 'P-2' 'A-8' 'A-9' 'F-3' 'M-6' 'M-1' 'M-2' 'S-2'\n",
      " 'P-10' 'T-4' 'T-5' 'F-7' 'M-3' 'M-4' 'M-5' 'P-15' 'C-1' 'C-2' 'T-12'\n",
      " 'T-13' 'F-4' 'F-5' 'D-14' 'T-9' 'P-14' 'T-8' 'P-11' 'D-15' 'D-16' 'M-7'\n",
      " 'F-8']\n"
     ]
    }
   ],
   "source": [
    "print(train_datas.keys())\n",
    "print('\\n',test_datas.keys())\n",
    "print('\\n',labels['chan_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 temporal correlation\n",
    "# in out sub block \n",
    "class InputSubBlock(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(InputSubBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(INPUT_LENGTH, 50)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 50)\n",
    "        self.fc4 = nn.Linear(50, 50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "# cascade sub block \n",
    "class CascadeSubBlock(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CascadeSubBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(50, 50)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(50, INPUT_LENGTH)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "#forecasting sub block\n",
    "class ForecastingSubBlock(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ForecastingSubBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(50, 50)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(50, INTERMEDIATE_LENGTH)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "#MLPBlock\n",
    "class MLPBlock(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.input = InputSubBlock()\n",
    "        self.cascade = CascadeSubBlock()\n",
    "        self.forecasting = ForecastingSubBlock()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"1\",x.shape)\n",
    "        x = self.input(x)\n",
    "        #print(\"2s\",x.shape)\n",
    "        cascade = self.cascade(x)\n",
    "        \n",
    "        forecast = self.forecasting(x)\n",
    "        \n",
    "        return cascade , forecast\n",
    "class TemporalModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TemporalModule, self).__init__()\n",
    "        self.mlp1 = MLPBlock() \n",
    "        self.mlp2 = MLPBlock() \n",
    "\n",
    "    def forward(self, input):\n",
    "        mlp1_out , forecast = self.mlp1(input)\n",
    "        new_input = mlp1_out - input \n",
    "        mlp2_out , forecast_2 = self.mlp2(new_input)\n",
    "        return (forecast + forecast_2)\n",
    "\n",
    "class SpatialModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialModule, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=INTERMEDIATE_LENGTH, num_heads=8, dropout=0.05)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(INTERMEDIATE_LENGTH, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.05),\n",
    "            nn.Linear(50,INTERMEDIATE_LENGTH)            \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x, _ = self.self_attention(x, x, x)\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "    \n",
    "class OutputModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OutputModule, self).__init__()\n",
    "        self.FFN = nn.Linear(INTERMEDIATE_LENGTH,1)\n",
    "    def forward(self, x):\n",
    "        x = self.FFN(x)\n",
    "        return x\n",
    "    \n",
    "class modeMARINA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modeMARINA, self).__init__()\n",
    "        self.temporal = TemporalModule()\n",
    "        self.spatial  = SpatialModule()\n",
    "        self.output   = OutputModule()\n",
    "    def forward(self, x):\n",
    "        x = self.temporal(x)\n",
    "        x = self.spatial(x)\n",
    "        x = self.output(x)\n",
    "        return x  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarinaDataset(Dataset):\n",
    "    def __init__(self, vec,label):\n",
    "        self.vec = vec\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return len(self.vec)\n",
    "    def __getitem__(self, idx):        \n",
    "        return self.vec[idx], self.label[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length of whole trainDataT: 26536 , and shape of input torch.Size([25, 100]) \n",
      "Length of whole futured (targets) list: 26536 , and shape of output torch.Size([25, 1])\n",
      "\n",
      "List length of whole testSet: 86104 , and shape of input torch.Size([25, 100]) \n",
      "Length of whole futured test (targets) list: 86104 , and shape of output torch.Size([25, 1])\n"
     ]
    }
   ],
   "source": [
    "# Stacking all data\n",
    "alltrainDataT      = []    # Create a list for training data\n",
    "allfuturedTrainT   = []    \n",
    "testSet          = []  \n",
    "test_futured       = [] \n",
    "\n",
    "testSetIdx         = []\n",
    "for i,name in enumerate(windowedTrain.keys()):\n",
    "    for j in range(windowedTrain[name].shape[1]):                                 # Looping over T dimension for stacking                              \n",
    "        alltrainDataT.append(torch.tensor(windowedTrain[name][:,0,:]).float())   # Swapping the dimensions accordingly as (T,D,W).\n",
    "        allfuturedTrainT.append(torch.tensor(futuredTrain[name][:,0,:]).float()) # Swapping the dimensions accordingly as (T,D,W).\n",
    "      \n",
    "for i,name in enumerate(windowedTest.keys()):\n",
    "    for j in range(windowedTest[name].shape[1]):                                 # Looping over T dimension for stacking                              \n",
    "        testSet.append(torch.tensor(windowedTest[name][:,0,:]).float())   # Swapping the dimensions accordingly as (T,D,W).\n",
    "        test_futured.append(torch.tensor(futuredTest[name][:,0,:]).float()) # Swapping the dimensions accordingly as (T,D,W).\n",
    "        testSetIdx.append([name, wL + zeta + shift*j - 1])\n",
    "\n",
    "    #print(windowedTrain[name].shape)\n",
    "print(\"List length of whole trainDataT:\",len(alltrainDataT),\", and shape of input\",alltrainDataT[1].shape,\n",
    "      \"\\nLength of whole futured (targets) list:\",len(allfuturedTrainT),\", and shape of output\",allfuturedTrainT[1].shape)\n",
    "print(\"\\nList length of whole testSet:\",len(testSet),\", and shape of input\",testSet[1].shape,\n",
    "      \"\\nLength of whole futured test (targets) list:\",len(test_futured),\", and shape of output\",test_futured[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data and futured:\n",
      "21229 21229\n",
      "Validation data and futured:\n",
      "5307 5307\n",
      "Test data and futured:\n",
      "86104 86104\n"
     ]
    }
   ],
   "source": [
    "# Generate random indices for the validation set\n",
    "valP            = 0.2 # Select 20% of the indices for validation\n",
    "val_indices = random.sample(range(len(alltrainDataT)), k=int(len(alltrainDataT) * valP))  \n",
    "# Split the data into training and validation sets\n",
    "trainSet, validSet, train_futured, val_futured = [], [], [], []\n",
    "for i in range(len(alltrainDataT)):\n",
    "    if i in val_indices:\n",
    "        validSet.append(alltrainDataT[i])\n",
    "        val_futured.append(allfuturedTrainT[i])\n",
    "    else:\n",
    "        trainSet.append(alltrainDataT[i])\n",
    "        train_futured.append(allfuturedTrainT[i])\n",
    "\n",
    "print(\"Training data and futured:\")\n",
    "print(len(trainSet),len(train_futured))\n",
    "print(\"Validation data and futured:\")\n",
    "print(len(validSet),len(val_futured))\n",
    "print(\"Test data and futured:\")\n",
    "print(len(testSet),len(test_futured))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training params\n",
    "BATCH_SIZE                  = 128\n",
    "lr                          = 0.005\n",
    "\n",
    "trainset            = MarinaDataset(trainSet,train_futured)\n",
    "trainloader         = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,shuffle=True)      \n",
    " \n",
    "validset            = MarinaDataset(validSet,val_futured)\n",
    "validloader         = torch.utils.data.DataLoader(validset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "testset            = MarinaDataset(testSet,test_futured)\n",
    "testloader         = torch.utils.data.DataLoader(testset, batch_size=1)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modeMARINA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model           \u001b[39m=\u001b[39m modeMARINA()\n\u001b[0;32m      2\u001b[0m model           \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m criterion       \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'modeMARINA' is not defined"
     ]
    }
   ],
   "source": [
    "model           = modeMARINA()\n",
    "model           = model.to(device)\n",
    "criterion       = nn.MSELoss()\n",
    "optimizer       = optim.Adam(model.parameters(), lr=lr)\n",
    "min_valid_loss  = np.inf\n",
    "epoch           = 30       # From the figure 7 in the paper.\n",
    "loss_history    = []\n",
    "\n",
    "start_time = time.time() # Setting starting point for finding the execution time\n",
    "\n",
    "for epoch in range(epoch):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        print(inputs.shape,labels.shape)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to(device)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    loss_history.append(running_loss / len(trainloader))\n",
    "    print(f'Epoch {epoch+1:2d} \\t Training Loss: {running_loss / len(trainloader):.6f} \\t Validation Loss: {valid_loss / len(validloader):.6f}')\n",
    "\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'\\t\\t Validation Loss Decreased ({min_valid_loss/len(validloader):.6f} --> {valid_loss/len(validloader):.6f})')\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict\n",
    "        # torch.save(model.state_dict(),'saved_model.pth')\n",
    "    \n",
    "print('Finished Training')\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total Execution Time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# Plotting the loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss History')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATING MSE\n",
    "def calculate_mse(model, testloader):\n",
    "    model.eval()\n",
    "    mse = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            mse += torch.mean((outputs - labels) ** 2).item()\n",
    "\n",
    "    mse /= len(testloader)\n",
    "\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.117456\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "mse_score = calculate_mse(model, testloader)\n",
    "print(f\"MSE: {mse_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86104 torch.Size([25, 1])\n",
      "86104 torch.Size([25, 100])\n"
     ]
    }
   ],
   "source": [
    "print(len(test_futured),test_futured[0].shape)\n",
    "print(len(testSet),testSet[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        for i in range(len(val)):\n",
    "            if val[i] == value:\n",
    "                return key\n",
    "    return None  # Value not found in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = test_datas.keys()\n",
    "default_value = []  # Default empty list\n",
    "anomalies = {key: default_value for key in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies[\"A-1.npy\"].append(2323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2132, 2132, 2323]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalies[\"A-1.npy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = 0\n",
    "for data in testloader:\n",
    "    if ll < 1:\n",
    "        inputs, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyNL,valueNL = [],[]\n",
    "keys = test_datas.keys()\n",
    "default_value = []  # Default empty list\n",
    "#anomalies = {key: default_value for key in keys}\n",
    "anomalies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies09 = {}\n",
    "[keyN, valueN] =  testSetIdx[1]\n",
    "anomalies09[keyN] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anomalies = {key: default_value for key in keys}\n",
    "anomalies = {}\n",
    "i = 0\n",
    "ii = 0\n",
    "threshold = 0.85\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        if ii < 8640:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            #compare output with labels using frobenius norm\n",
    "            frob_error  =  torch.sqrt(torch.abs(torch.sum(outputs ** 2 - labels ** 2)))\n",
    "            #this if else is for catching the beginning and end of the sequences\n",
    "            if( frob_error > threshold ):\n",
    "                [keyN, valueN] =  testSetIdx[ii]\n",
    "                if keyN not in anomalies:\n",
    "                    anomalies[keyN] = []\n",
    "                anomalies[keyN].append(valueN)\n",
    "            ii += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'A-1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[428], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m anomalies[\u001b[39m\"\u001b[39;49m\u001b[39mA-1.npy\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'A-1.npy'"
     ]
    }
   ],
   "source": [
    "anomalies[\"A-1.npy\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b369e7963f7abcd480fad95d5beb7b146f8ec315fde1b361e4f21d5ea962be01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
